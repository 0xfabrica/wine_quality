# Clasificaci√≥n de calidad de vino (Notebook)

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![pandas](https://img.shields.io/badge/pandas-%3E%3D1.3-0f4c81)](https://pandas.pydata.org/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-%3E%3D1.0-orange)](https://scikit-learn.org/)
[![xgboost](https://img.shields.io/badge/xgboost-%3E%3D1.6-red)](https://xgboost.readthedocs.io/)
[![imbalanced-learn](https://img.shields.io/badge/imbalanced--learn-SMOTE-lightgrey)](https://imbalanced-learn.org/)
[![matplotlib](https://img.shields.io/badge/matplotlib-%3E%3D3.4-CC2927)](https://matplotlib.org/)
[![seaborn](https://img.shields.io/badge/seaborn-%3E%3D0.11-purple)](https://seaborn.pydata.org/)
[![plotly](https://img.shields.io/badge/plotly-%3E%3D5.0-blueviolet)](https://plotly.com/)
[![Jupyter](https://img.shields.io/badge/Jupyter-notebook-orange)](https://jupyter.org/)

## Resumen

Este repositorio contiene un notebook que explora, procesa y modela el dataset de vino tinto (Wine Quality - red) para predecir la calidad del vino. Se realiza un EDA visual, se agrupan las etiquetas de `quality` en tres clases (`Baja`, `Media`, `Alta`), se corrige el desbalance con SMOTE y se comparan dos modelos: Random Forest y XGBoost. El objetivo es entregar un pipeline reproducible y recomendaciones pr√°cticas para uso por en√≥logos o como proof-of-concept en proyectos ML.

---

## Estructura del repositorio

```
üìÅ repo/
  ‚îú‚îÄ xgb_model.json                    # modelo XGBoost guardado
  ‚îú‚îÄ notebook.ipynb                    # notebook principal
  ‚îú‚îÄ requirements.txt                  # dependencias del proyecto
  ‚îî‚îÄ README.md                         # documento que est√°s leyendo
```

---

## Dataset

El dataset original no se incluye en este repositorio debido a su tama√±o. Puedes descargarlo directamente desde el repositorio oficial de UCI Machine Learning:

üîó [Wine Quality Dataset (UCI)](https://archive.ics.uci.edu/dataset/186/wine+quality)

Archivo a usar: **winequality-red.csv** (separador `;`).

Col√≥calo en la misma carpeta ra√≠z del proyecto o ajusta la ruta en el notebook.

---

## Requisitos (entorno)

Se recomienda crear un entorno virtual (venv/conda) y usar Python 3.8+. Un `requirements.txt` b√°sico ya est√° incluido en la ra√≠z del proyecto:

```
pandas
numpy
matplotlib
seaborn
scikit-learn
xgboost
imblearn
joblib
```

Instalaci√≥n r√°pida:

```bash
python -m venv .venv
source .venv/bin/activate   # o .venv\Scripts\activate en Windows
pip install -r requirements.txt
```

---

## Pipeline del Notebook (pasos principales)

1. **Carga de librer√≠as y dataset**
2. **EDA**: scatter plots, series por √≠ndice, heatmap de correlaci√≥n.
3. **Selecci√≥n de features**
4. **Separaci√≥n train/test**
5. **Reagrupaci√≥n de etiquetas (Baja/Media/Alta)**
6. **Escalado con StandardScaler**
7. **Balanceo con SMOTE**
8. **Modelado**: RandomForest (baseline) y XGBoost (modelo final)
9. **Evaluaci√≥n**: accuracy, classification\_report, confusion\_matrix
10. **Guardado de modelo**: `xgb_model.json`

---

## Resultados resumen

* **Random Forest (baseline)**

  * Accuracy: \~0.68
  * Weighted F1: \~0.70
  * D√©bil en clase `Baja`.

* **XGBoost (modelo final)**

  * Accuracy: \~0.765
  * Weighted F1: \~0.77
  * Fuerte en clase `Alta`, estable en `Media`, m√°s o menos pobre en `Baja` por escasez de datos.

**Interpretaci√≥n**: XGBoost es la mejor opci√≥n actual para predecir vinos de calidad `Alta` y `Media`, la `Baja` es algo peor.

---

## Pr√≥ximos pasos recomendados

1. Probar nuevas features o transformaciones.
2. Implementar una API (FastAPI/Flask) para servir el modelo.

---

## Matriz de Confusi√≥n

<img width="649" height="547" alt="download" src="https://github.com/user-attachments/assets/074c9995-2150-44c1-abd4-d714825706da" />

---


## Archivos generados

* `modelos/xgb_model.json` ‚Äî modelo XGBoost entrenado.

---

## Licencia

MIT License ‚Äî √∫salo como base para tus experimentos y a√±ade atribuciones si compartes resultados.
